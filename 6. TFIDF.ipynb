{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "DATA_PATH = Path('./dat/')\n",
    "DATA_PATH.mkdir(exist_ok =True)\n",
    "#if not os.path.exists('./dat/aclImdb_v1.tar.gz'):\n",
    "if not os.path.exists('./dat/aclImdb'):\n",
    "    !curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xf aclImdb_v1.tar.gz -C {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASSES = ['neg' ,'pos'] #,'unsup']\n",
    "PATH=Path('./dat/aclImdb/')\n",
    "def get_texts(path):\n",
    "  texts ,labels = [] ,[]\n",
    "  for idx , label in enumerate(CLASSES):\n",
    "    for fname in (path/label).glob('*.*'):\n",
    "      #texts.append(fixup(fname.open('r',encoding='utf-8').read()))\n",
    "      texts.append(fname.open('r', encoding='utf-8').read())\n",
    "      labels.append(idx)\n",
    "      #return np.array(texts), np.array(labels)\n",
    "      return texts , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text , train_labels =get_texts(PATH/'train')\n",
    "test_text  , test_labels  =get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\",\n",
       " \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = train_text + test_text\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counts suffer some issues: most frequent words are usually not important (like stop words), while they take high focus/count.\n",
    "\n",
    "TFIDF (Term Frequency - Inverse Document) is a way to adjust those counts:\n",
    "\n",
    "- TF: #mentions within a document\n",
    "- IDF: #mentions across all docs (same as the counts before)\n",
    "\n",
    "So it gives higher importance to rare words across all docs (IDF++, TFIDF--), while it emphasyses on words appearing mostly in THIS doc (TF++, TFIDF--).\n",
    "\n",
    "If a word appearing only in the current doc/sentence, it has TFIDF=1.\n",
    "If a word appears in all docs/sents but not the current one, it has TFIDF=0.\n",
    "If a word (stop word for example) appearing a lot in the current doc/sent and also in ALL others, it will have high TF (count) and much higher IDF (discount), so low TFIDF overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 178)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "bow = vectorizer.transform(texts[:10])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.18689793, 0.        , 0.        , 0.06648971,\n",
       "        0.06648971, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.09344896,\n",
       "        0.13297941, 0.        , 0.06648971, 0.09344896, 0.13297941,\n",
       "        0.09344896, 0.        , 0.09344896, 0.        , 0.        ,\n",
       "        0.09344896, 0.        , 0.        , 0.        , 0.09344896,\n",
       "        0.        , 0.        , 0.        , 0.09344896, 0.09344896,\n",
       "        0.09344896, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09344896, 0.        , 0.09344896, 0.09344896,\n",
       "        0.09344896, 0.09344896, 0.        , 0.09344896, 0.        ,\n",
       "        0.        , 0.06648971, 0.        , 0.        , 0.09344896,\n",
       "        0.09344896, 0.09344896, 0.06648971, 0.18689793, 0.09344896,\n",
       "        0.        , 0.09344896, 0.09344896, 0.09344896, 0.        ,\n",
       "        0.06648971, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.09344896,\n",
       "        0.09344896, 0.13297941, 0.26595883, 0.06648971, 0.        ,\n",
       "        0.        , 0.09344896, 0.        , 0.        , 0.09344896,\n",
       "        0.        , 0.        , 0.09344896, 0.09344896, 0.09344896,\n",
       "        0.09344896, 0.09344896, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09344896, 0.        , 0.06648971, 0.        ,\n",
       "        0.        , 0.19946912, 0.13297941, 0.06648971, 0.        ,\n",
       "        0.        , 0.09344896, 0.        , 0.09344896, 0.06648971,\n",
       "        0.        , 0.        , 0.        , 0.09344896, 0.        ,\n",
       "        0.        , 0.        , 0.09344896, 0.        , 0.        ,\n",
       "        0.        , 0.09344896, 0.09344896, 0.        , 0.09344896,\n",
       "        0.09344896, 0.        , 0.09344896, 0.06648971, 0.        ,\n",
       "        0.        , 0.09344896, 0.09344896, 0.09344896, 0.09344896,\n",
       "        0.09344896, 0.        , 0.09344896, 0.09344896, 0.        ,\n",
       "        0.06648971, 0.06648971, 0.09344896, 0.26595883, 0.        ,\n",
       "        0.        , 0.09344896, 0.        , 0.09344896, 0.        ,\n",
       "        0.09344896, 0.06648971, 0.06648971, 0.        , 0.09344896,\n",
       "        0.18689793, 0.        , 0.09344896, 0.09344896, 0.        ,\n",
       "        0.        , 0.        , 0.09344896, 0.09344896, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06648971, 0.09344896, 0.        , 0.        , 0.19946912,\n",
       "        0.09344896, 0.09344896, 0.09344896],\n",
       "       [0.18699198, 0.        , 0.06233066, 0.12466132, 0.04434878,\n",
       "        0.13304633, 0.06233066, 0.06233066, 0.06233066, 0.24932264,\n",
       "        0.06233066, 0.06233066, 0.06233066, 0.06233066, 0.        ,\n",
       "        0.08869756, 0.06233066, 0.04434878, 0.        , 0.04434878,\n",
       "        0.        , 0.18699198, 0.        , 0.12466132, 0.06233066,\n",
       "        0.        , 0.12466132, 0.06233066, 0.06233066, 0.        ,\n",
       "        0.06233066, 0.24932264, 0.06233066, 0.        , 0.        ,\n",
       "        0.        , 0.12466132, 0.06233066, 0.06233066, 0.06233066,\n",
       "        0.06233066, 0.        , 0.06233066, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06233066, 0.        , 0.06233066,\n",
       "        0.06233066, 0.04434878, 0.06233066, 0.06233066, 0.        ,\n",
       "        0.        , 0.        , 0.08869756, 0.        , 0.        ,\n",
       "        0.12466132, 0.        , 0.        , 0.        , 0.06233066,\n",
       "        0.04434878, 0.06233066, 0.12466132, 0.06233066, 0.06233066,\n",
       "        0.06233066, 0.06233066, 0.12466132, 0.06233066, 0.        ,\n",
       "        0.        , 0.13304633, 0.08869756, 0.04434878, 0.06233066,\n",
       "        0.06233066, 0.        , 0.18699198, 0.06233066, 0.        ,\n",
       "        0.06233066, 0.06233066, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06233066, 0.06233066, 0.06233066,\n",
       "        0.06233066, 0.        , 0.06233066, 0.13304633, 0.12466132,\n",
       "        0.06233066, 0.22174389, 0.08869756, 0.04434878, 0.06233066,\n",
       "        0.06233066, 0.        , 0.06233066, 0.        , 0.04434878,\n",
       "        0.06233066, 0.06233066, 0.06233066, 0.        , 0.06233066,\n",
       "        0.06233066, 0.06233066, 0.        , 0.06233066, 0.06233066,\n",
       "        0.06233066, 0.        , 0.        , 0.06233066, 0.        ,\n",
       "        0.        , 0.06233066, 0.        , 0.04434878, 0.06233066,\n",
       "        0.06233066, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06233066, 0.        , 0.        , 0.06233066,\n",
       "        0.04434878, 0.08869756, 0.        , 0.31044145, 0.06233066,\n",
       "        0.06233066, 0.        , 0.06233066, 0.        , 0.06233066,\n",
       "        0.        , 0.04434878, 0.13304633, 0.06233066, 0.        ,\n",
       "        0.        , 0.06233066, 0.        , 0.        , 0.06233066,\n",
       "        0.12466132, 0.12466132, 0.        , 0.        , 0.06233066,\n",
       "        0.06233066, 0.18699198, 0.06233066, 0.06233066, 0.12466132,\n",
       "        0.04434878, 0.        , 0.06233066, 0.06233066, 0.04434878,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "\n",
    "- A vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector. \n",
    "\n",
    "- The inverse document frequencies are calculated for each\n",
    "word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: the at index 7. \n",
    "\n",
    "- Finally, the first document is encoded as an 8-element sparse array and we can\n",
    "review the final scorings of each word with di\u000berent values for the, fox, and dog from the otherwords in the vocabulary.\n",
    "\n",
    "Notice how the words the, do, fox have the lowest scores, since they are mentioned in all the 3 docs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all the pipeline together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x1))\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    \"\"\"\n",
    "    :param words:\n",
    "    :type words:\n",
    "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    or\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    :type stop_words:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize words in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    #words = stem_words(words)# Either stem ovocar lemmatize\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize_text(trn_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this on the whole corpus:d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus):\n",
    "      return [normalize_text(t) for t in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def normalize_corpus(corpus):\n",
    " #     return [normalize_text(t) for t in corpus]\n",
    "#tst_texts = normalize_corpus(tst_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "for t in trn_texts[:10]:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting gll together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "tst_texts,tst_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trn_texts \u001b[39m=\u001b[39m normalize_corpus(trn_texts)\n\u001b[0;32m      2\u001b[0m tst_texts \u001b[39m=\u001b[39m normalize_corpus(tst_texts)\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_corpus\u001b[39m(corpus):\n\u001b[1;32m----> 2\u001b[0m       \u001b[39mreturn\u001b[39;00m [normalize_text(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m corpus]\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_corpus\u001b[39m(corpus):\n\u001b[1;32m----> 2\u001b[0m       \u001b[39mreturn\u001b[39;00m [normalize_text(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m corpus]\n",
      "Cell \u001b[1;32mIn[11], line 70\u001b[0m, in \u001b[0;36mnormalize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_text\u001b[39m( text):\n\u001b[1;32m---> 70\u001b[0m     text \u001b[39m=\u001b[39m remove_special_chars(text)\n\u001b[0;32m     71\u001b[0m     text \u001b[39m=\u001b[39m remove_non_ascii(text)\n\u001b[0;32m     72\u001b[0m     text \u001b[39m=\u001b[39m remove_punctuation(text)\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mremove_special_chars\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_special_chars\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     re1 \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m  +\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     x1 \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m#39;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mamp;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m&\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m#146;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\n\u001b[0;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnbsp;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m#36;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mquot;\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\n\u001b[0;32m      5\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m<br />\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mu_n\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m @.@ \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\n\u001b[0;32m      6\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m @-@ \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m re1\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, html\u001b[39m.\u001b[39munescape(x1))\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "trn_texts = normalize_corpus(trn_texts)\n",
    "tst_texts = normalize_corpus(tst_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW\n",
    "\n",
    "### Binary features\n",
    "\n",
    "\n",
    "We will use the tokenizer of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Either pre-define vocab size\n",
    "# Or get the max possible vocab from text\n",
    "vocab_sz = 10000 # None means all\n",
    "tok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\n",
    "#tok = Tokenizer(oov_token='UNK')\n",
    "tok.fit_on_texts(trn_texts + tst_texts)\n",
    "#vocab_sz = len(tok.word_index) # If all possible vocab, else, it's the predefine vocab_sz. Remember we cannot always use the len(tok.word_index), since it's always the max.\n",
    "\n",
    "# Extract binary BoW features\n",
    "x_train = tok.texts_to_matrix(trn_texts, mode='binary')\n",
    "x_test = tok.texts_to_matrix(tst_texts, mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10000)\n",
      "(1,)\n",
      "(1, 10000)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(trn_labels).astype('float32')\n",
    "y_test = np.asarray(tst_labels).astype('float32')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                160016    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(partial_x_train,\n\u001b[0;32m      2\u001b[0m                     partial_y_train,\n\u001b[0;32m      3\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(x_val, y_val))\n",
      "File \u001b[1;32mc:\\Users\\Alhou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Alhou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1395\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1393\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1394\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnexpected result of `train_function` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1396\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m(Empty logs). Please use \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1397\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1398\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1399\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1400\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1401\u001b[0m epoch_logs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(logs)\n\u001b[0;32m   1403\u001b[0m \u001b[39m# Run validation.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_dict \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory\n\u001b[0;32m      2\u001b[0m history_dict\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m      5\u001b[0m acc \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mbinary_accuracy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
