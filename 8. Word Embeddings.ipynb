{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the effect is minimal, but the val curve a bit smoother in case of text clean-up\n",
    "\n",
    "# Why?\n",
    "\n",
    "We already limit the vocab to the most frequent 10K words, and we use BoW features, so cleanining up the text has minimal effect, since the words context is neglected (high frequent words are likely to always be scored no matter different morphologies might be missed sometimes).\n",
    "\n",
    "For sequence models for example with word embeddings (as we will see), the context of the word will have strong effect, so the clean up step will have more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (701).png](<attachment:Screenshot (701).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-word Vectors:\n",
    "\n",
    "Th above model suffers two issues:\n",
    "\n",
    "1- Sequence info is lost. We will treat that later with sequence models\n",
    "\n",
    "2- __Sparsity__: the input scalars represent words indices. Those are sparse, and do not include any info about the word (is it a verb, is it a noun, is it a sarcastic word,....etc). Also, this input is very sparse, where in a sentence, we only have few words of the vocab.\n",
    "\n",
    "Thus, we use `Embedding` layer, that will encode _latent_ factors of the word features. In the current setup, we will _learn_ what those latent features are during training. \n",
    "\n",
    "## Embedding\n",
    "\n",
    "The input scalars represent words indices. Those are sparse, and do not include any info about the word (is it a verb, is it a noun, is it a sarcastic word,....etc). Also, this input is very sparse, where in a sentence, we only have few words of the vocab.\n",
    "\n",
    "Thus, we use `Embedding` layer, that will encode _latent_ factors of the word features. In the current setup, we will _learn_ what those latent features are during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (702).png](<attachment:Screenshot (702).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "A word embedding is a class of approaches for representing words and documents using a\n",
    "dense vector representation. It is an improvement over more the traditional bag-of-word model\n",
    "encoding schemes where large sparse vectors were used to represent each word or to score each\n",
    "word within a vector to represent an entire vocabulary. These representations were sparse\n",
    "because the vocabularies were vast and a given word or document would be represented by a\n",
    "large vector comprised mostly of zero values.\n",
    "Instead, in an embedding, words are represented by dense vectors where a vector represents\n",
    "the projection of the word into a continuous vector space. The position of a word within the\n",
    "vector space is learned from text and is based on the words that surround the word when it is\n",
    "used. The position of a word in the learned vector space is referred to as its embedding. Two\n",
    "popular examples of methods of learning word embeddings from text include:\n",
    "\n",
    "- Word2Vec.\n",
    "- GloVe.\n",
    "In addition to these carefully designed methods, a word embedding can be learned as part\n",
    "of a deep learning model. This can be a slower approach, but tailors the model to a speci\fc\n",
    "training dataset.\n",
    "* \f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (700).png](<attachment:Screenshot (700).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Embedding Layer\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data. \n",
    "\n",
    "You can think of it as:\n",
    "- LUT\n",
    "- Weight Matrix: n_vocab x emb_sz\n",
    "\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer.\n",
    "\n",
    "This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. \n",
    "\n",
    "It is a layer that can be used in a variety of ways, such as:\n",
    "\n",
    "- _From scratch embeddings_ It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "\n",
    "- _Learnable embeddings_ It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "\n",
    "- _Pre-trained embeddings_ It can be used to load a pre-trained word embedding model, a type of transfer learning. This can be mixed with learnable embeddings, where those weights are later updated or frozen as part of the larger model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "- input dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "- output dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "-  input length: This is the length of input sequences, as you would de\fne for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (704).png](<attachment:Screenshot (704).png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding \n",
    "e = Embedding(200 , 32 , input_length = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer. \n",
    "\n",
    "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document). \n",
    "\n",
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer. \n",
    "\n",
    "Now, let's see how we can use an Embedding layer in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "* we will look at how we can learn a word embedding while setting a neural network on a text classifiation problem.\n",
    "*  We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. \n",
    "\n",
    "* Each text document is classified as positive 1 or negative 0. \n",
    "* This is a simple sentiment analysis problem. \n",
    "* First, we will de\fne the documents and their class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **pad_sequences is a function in the Keras deep learning library that is used to pad sequences to a specific length.**\n",
    "* **It is commonly used when working with sequences of variable lengths, such as in natural language processing tasks.**\n",
    "![Screenshot (706).png](<attachment:Screenshot (706).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **A tokenizer is a tool used in natural language processing (NLP) to break text into smaller units called tokens.**\n",
    "* **These tokens can be words, sentences, or even individual characters, depending on the requirements of the task at hand.**\n",
    "* **Tokenization is a crucial step in many NLP tasks such as text classification, named entity recognition, and machine translation.**\n",
    "![Screenshot (707).png](<attachment:Screenshot (707).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imported Libraries are Used in a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten , Dense , Dropout \n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Padding = 'Post'`\n",
    "\n",
    "![Screenshot (708).png](<attachment:Screenshot (708).png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tokenizer Is -->  <keras_preprocessing.text.Tokenizer object at 0x000002B1CFDB08E0>\n",
      "********************************************************************************\n",
      "The length of vocabulary size depend on length of tokenizer for (word index) Is-->  15\n",
      "********************************************************************************\n",
      "The Encoded Docs For Each Tokens Is Sentence Is -->  [[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "********************************************************************************\n",
      "\t\t Padding Docs For Max_Length = 4 \n",
      " [[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# define documents\n",
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!','Weak','Poor effort!','not good','poor work','Could have done better.']\n",
    "#-----------------------------------------------------------------------\n",
    "# define class labels \n",
    "labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "#-----------------------------------------------------------------------\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "print('The Tokenizer Is --> ',t)\n",
    "print('**'*40)\n",
    "#-------------------------------------------------------------------\n",
    "vocab_size = len(t.word_index )+ 1\n",
    "print('The length of vocabulary size depend on length of tokenizer for (word index) Is--> ',vocab_size)\n",
    "print('**'*40)\n",
    "#--------------------------------------------------------------------\n",
    "# integer encode the documents\n",
    "encode_docs = t.texts_to_sequences(docs)\n",
    "print('The Encoded Docs For Each Tokens Is Sentence Is --> ',encode_docs)\n",
    "print('**'*40)\n",
    "#---------------------------------------------------------------------\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encode_docs ,maxlen=max_length , padding='post')\n",
    "print('\\t\\t Padding Docs For Max_Length = 4 \\n',padded_docs)\n",
    "print('**'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (709).png](<attachment:Screenshot (709).png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m embedding_index \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m() \u001b[39m# Create Emprt Dictionary \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#Open File \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mglove.6B.100d.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39m#Create for loop because read for each word / tokens in file and make splitting \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f : \u001b[39m# F --> File \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "## load the whole embedding into memory\n",
    "import numpy as np \n",
    "embedding_index = dict() # Create Emprt Dictionary \n",
    "#Open File \n",
    "f = open('glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
    "#Create for loop because read for each word / tokens in file and make splitting \n",
    "for line in f : # F --> File \n",
    "    values = line.split()#Vaues Conatin Splitting For Line \n",
    "    word = values[0] # Word --> Is strat from index 0 / token 0\n",
    "    coefs = np.asarray(values[1:] , dtype = 'float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 100)            1500      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     17\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model\u001b[39m.\u001b[39;49mfit(padded_docs, labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(padded_docs, labels, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alhou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Alhou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:984\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    981\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    982\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    983\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    987\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[0;32m    988\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "  embedding_vector = embedding_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
