{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "DATA_PATH = Path('./dat/')\n",
    "DATA_PATH.mkdir(exist_ok =True)\n",
    "#if not os.path.exists('./dat/aclImdb_v1.tar.gz'):\n",
    "if not os.path.exists('./dat/aclImdb'):\n",
    "    !curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xf aclImdb_v1.tar.gz -C {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASSES = ['neg' ,'pos'] #,'unsup']\n",
    "PATH=Path('./dat/aclImdb/')\n",
    "def get_texts(path):\n",
    "  texts ,labels = [] ,[]\n",
    "  for idx , label in enumerate(CLASSES):\n",
    "    for fname in (path/label).glob('*.*'):\n",
    "      #texts.append(fixup(fname.open('r',encoding='utf-8').read()))\n",
    "      texts.append(fname.open('r', encoding='utf-8').read())\n",
    "      labels.append(idx)\n",
    "      #return np.array(texts), np.array(labels)\n",
    "      return texts , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.datasets import imdb\n",
    "#(train_text, train_labels), (test_text, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text , train_labels =get_texts(PATH/'train')\n",
    "test_text  , test_labels  =get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\",\n",
       " \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "texts = train_text + test_text\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important notes on Keras Tokenizer:\n",
    "\n",
    "- As you can see, the UNK token is added with index = 1. Index 0 is never given.\n",
    "\n",
    "This info is important when dealing later with Embedding layer in Keras, where padding with 0's might cause OOV if not considered, so we init that layer with vocab_size + 1 (to account for the missing 0). That layer also has option to mask_zero if needed for LSTM.\n",
    "\n",
    "Another solution would be to -1 from all binarized values. But this would make padded words same as UNK.\n",
    "\n",
    "In other words, it's a choice to have two different values for:\n",
    "\n",
    "A- UNK-->1\n",
    "B- PAD-->0\n",
    "\n",
    "In this case, the vocab_sz should increase by 1 (for pad), in case the model needs padding like LSTM.\n",
    "\n",
    "Or keep them both as 0. In this case, the vocab_sz remains the same as given from the tokenizer.\n",
    "\n",
    "- If you fit a tokenizer wil small vocab using num_words, then take care that the word_index will still hold the full vocab.Then if you vectorize the text, either use tokenizer.texts_to_sequences, which will take care of the passed num_words,or if you develop ur own str2idx dict, then take care of the vocab not to use word_index as is, but limit it with the num_words, __keeping in mind that word_index is ordered by freqeuency__. A better approach is to build your own vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words= 100 , oov_token = 'UNK')\n",
    "tok.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras tokenizer can be used to vectorize or binarize words into integer indices (more on that later). It takes list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1], [1], [1], [1], [], [1], [1], [1], [1], [1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Hello World'\n",
    "tok.texts_to_sequences(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened above is that texts_to_sequences considered s as a list of chars, ALL OOV so unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Hello World'\n",
    "tok.texts_to_sequences(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello is not in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to vectorize a sentence with word beyond the first 100 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'cartoon movie'\n",
    "tok.texts_to_sequences(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although movie is in the word_index, but it's considered unknown with texts_to_sequences. So we have to take care of that when dealing the vocab.\n",
    "\n",
    "\n",
    "Let's fit on all data and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [113]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(oov_token='UNK')\n",
    "tok.fit_on_texts(texts)\n",
    "s = 'cartoon movie'\n",
    "l = tok.texts_to_sequences(s.split())\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the word cartoon is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the shape is not as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(l).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect (1,2), for 1 sentence and 2 words.\n",
    "\n",
    "texts_to_sequences takes a list (char, words, sequences). So in the above case, it considers the 2 words as separate sentences.\n",
    "\n",
    "To fix this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 113]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tok.texts_to_sequences([s.split()])\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get back to text = decode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK movie']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.sequences_to_texts(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer can also be used for few more things:\n",
    "\n",
    "- word counts: A dictionary of words and their counts.\n",
    "\n",
    "- word docs: An integer count of the total number of documents that were used to fit the Tokenizer.\n",
    "\n",
    "- word index: A dictionary of words and their uniquely assigned integers.\n",
    "\n",
    "- document count: A dictionary of words and how many documents each appeared in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "The vectors we obtain are generally not of equal lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 113, 1], [1, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['cartoon movie show', 'hello world']\n",
    "docs = [s.split() for s in docs]\n",
    "l = tok.texts_to_sequences(docs)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most supervised learning models, we need the design matrix X to have fixed number of columns and number of rows as the data size. RNNs are exceptions, but in most frameworks, it's also required to have fixed length vectors.\n",
    "\n",
    "For that, we might need to pad the sequences to max len. For RNNs, the padded parts can be ignored later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 900)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = max([len(t) for t in texts])\n",
    "\n",
    "l = np.array(pad_sequences(l,\n",
    "                          maxlen=maxlen,\n",
    "                          padding='post',\n",
    "                          truncating='post'))\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 113,   1, ...,   0,   0,   0],\n",
       "       [  1,   1,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Padding can be performed on texts before or after binarization. However, it's better to be done after binarization since we pad 0's which won't be understood by the vocab during binarization\n",
    "\n",
    "- The padded vector is very sparse! This is due to exceptionally long sentences --> outliers. So it's better to remove them, or even limit the max sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text features\n",
    "\n",
    "So far, we have tranformed the text into binary/digital form that can be understood by ML models.\n",
    "\n",
    "However, we can further apply or extract different features from the vectorized form.\n",
    "\n",
    "In other words, we can represent the sequence of word indices we obtained in different forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with keras Tokenizer\n",
    "\n",
    "We can use keras tokenizer to build a simple BoW.\n",
    "\n",
    "The rows = number of sentences/documents\n",
    "\n",
    "The #columns = number of words in the vocab\n",
    "\n",
    "Each entry can encode different modes:\n",
    "\n",
    "- binary: Whether or not each word is present in the document. This is the default.\n",
    "\n",
    "- count: The count of each word in the document.\n",
    "\n",
    "- tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document (more on that later).\n",
    "\n",
    "- freq: The frequency of each word as a ratio of words within each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 186)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(oov_token='UNK')\n",
    "tok.fit_on_texts(texts)\n",
    "bow = tok.texts_to_matrix(texts[:10], mode='count')\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(num_words=100, oov_token='UNK')\n",
    "tok.fit_on_texts(texts)\n",
    "bow = tok.texts_to_matrix(texts[:10], mode='count')\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  9.,  4.,  7.,  3.,  2.,  3.,  2.,  1.,  2.,  2.,  1.,  1.,\n",
       "         0.,  2.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  2.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 0., 77.,  7.,  3.,  5.,  3.,  1.,  2.,  3.,  2.,  2.,  3.,  3.,\n",
       "         4.,  1.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with sklearn\n",
    "\n",
    "The BoW model above can be also produced using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 178)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "bow = vectorizer.transform(texts[:10])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': 137,\n",
       " 'of': 101,\n",
       " 'man': 89,\n",
       " 'who': 170,\n",
       " 'has': 65,\n",
       " 'unnatural': 158,\n",
       " 'feelings': 48,\n",
       " 'for': 51,\n",
       " 'pig': 113,\n",
       " 'starts': 134,\n",
       " 'out': 109,\n",
       " 'with': 174,\n",
       " 'opening': 106,\n",
       " 'scene': 122,\n",
       " 'that': 142,\n",
       " 'is': 76,\n",
       " 'terrific': 140,\n",
       " 'example': 46,\n",
       " 'absurd': 1,\n",
       " 'comedy': 29,\n",
       " 'formal': 54,\n",
       " 'orchestra': 108,\n",
       " 'audience': 14,\n",
       " 'turned': 155,\n",
       " 'into': 75,\n",
       " 'an': 4,\n",
       " 'insane': 74,\n",
       " 'violent': 163,\n",
       " 'mob': 91,\n",
       " 'by': 19,\n",
       " 'the': 143,\n",
       " 'crazy': 33,\n",
       " 'chantings': 22,\n",
       " 'it': 77,\n",
       " 'singers': 131,\n",
       " 'unfortunately': 157,\n",
       " 'stays': 135,\n",
       " 'whole': 171,\n",
       " 'time': 151,\n",
       " 'no': 98,\n",
       " 'general': 59,\n",
       " 'narrative': 96,\n",
       " 'eventually': 45,\n",
       " 'making': 88,\n",
       " 'just': 78,\n",
       " 'too': 154,\n",
       " 'off': 102,\n",
       " 'putting': 117,\n",
       " 'even': 44,\n",
       " 'those': 150,\n",
       " 'from': 57,\n",
       " 'era': 43,\n",
       " 'should': 128,\n",
       " 'be': 15,\n",
       " 'cryptic': 34,\n",
       " 'dialogue': 35,\n",
       " 'would': 175,\n",
       " 'make': 87,\n",
       " 'shakespeare': 127,\n",
       " 'seem': 124,\n",
       " 'easy': 41,\n",
       " 'to': 152,\n",
       " 'third': 148,\n",
       " 'grader': 62,\n",
       " 'on': 103,\n",
       " 'technical': 138,\n",
       " 'level': 84,\n",
       " 'better': 17,\n",
       " 'than': 141,\n",
       " 'you': 176,\n",
       " 'might': 90,\n",
       " 'think': 146,\n",
       " 'some': 132,\n",
       " 'good': 61,\n",
       " 'cinematography': 25,\n",
       " 'future': 58,\n",
       " 'great': 63,\n",
       " 'vilmos': 162,\n",
       " 'zsigmond': 177,\n",
       " 'stars': 133,\n",
       " 'sally': 121,\n",
       " 'kirkland': 81,\n",
       " 'and': 5,\n",
       " 'frederic': 56,\n",
       " 'forrest': 55,\n",
       " 'can': 20,\n",
       " 'seen': 125,\n",
       " 'briefly': 18,\n",
       " 'once': 104,\n",
       " 'again': 2,\n",
       " 'mr': 94,\n",
       " 'costner': 31,\n",
       " 'dragged': 38,\n",
       " 'movie': 93,\n",
       " 'far': 47,\n",
       " 'longer': 85,\n",
       " 'necessary': 97,\n",
       " 'aside': 13,\n",
       " 'sea': 123,\n",
       " 'rescue': 120,\n",
       " 'sequences': 126,\n",
       " 'which': 169,\n",
       " 'there': 145,\n",
       " 'are': 9,\n",
       " 'very': 161,\n",
       " 'few': 49,\n",
       " 'did': 36,\n",
       " 'not': 99,\n",
       " 'care': 21,\n",
       " 'about': 0,\n",
       " 'any': 6,\n",
       " 'characters': 24,\n",
       " 'most': 92,\n",
       " 'us': 160,\n",
       " 'have': 66,\n",
       " 'ghosts': 60,\n",
       " 'in': 72,\n",
       " 'closet': 26,\n",
       " 'character': 23,\n",
       " 'realized': 118,\n",
       " 'early': 40,\n",
       " 'then': 144,\n",
       " 'forgotten': 53,\n",
       " 'until': 159,\n",
       " 'much': 95,\n",
       " 'later': 83,\n",
       " 'we': 166,\n",
       " 'really': 119,\n",
       " 'cocky': 28,\n",
       " 'overconfident': 111,\n",
       " 'ashton': 12,\n",
       " 'kutcher': 82,\n",
       " 'problem': 116,\n",
       " 'he': 67,\n",
       " 'comes': 30,\n",
       " 'as': 11,\n",
       " 'kid': 80,\n",
       " 'thinks': 147,\n",
       " 'anyone': 7,\n",
       " 'else': 42,\n",
       " 'around': 10,\n",
       " 'him': 69,\n",
       " 'shows': 129,\n",
       " 'signs': 130,\n",
       " 'cluttered': 27,\n",
       " 'his': 70,\n",
       " 'only': 105,\n",
       " 'obstacle': 100,\n",
       " 'appears': 8,\n",
       " 'winning': 173,\n",
       " 'over': 110,\n",
       " 'finally': 50,\n",
       " 'when': 168,\n",
       " 'well': 167,\n",
       " 'past': 112,\n",
       " 'half': 64,\n",
       " 'way': 165,\n",
       " 'point': 114,\n",
       " 'this': 149,\n",
       " 'stinker': 136,\n",
       " 'tells': 139,\n",
       " 'all': 3,\n",
       " 'told': 153,\n",
       " 'why': 172,\n",
       " 'driven': 39,\n",
       " 'best': 16,\n",
       " 'prior': 115,\n",
       " 'inkling': 73,\n",
       " 'or': 107,\n",
       " 'foreshadowing': 52,\n",
       " 'magic': 86,\n",
       " 'here': 68,\n",
       " 'was': 164,\n",
       " 'could': 32,\n",
       " 'do': 37,\n",
       " 'keep': 79,\n",
       " 'turning': 156,\n",
       " 'hour': 71}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x178 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 178)\n",
      "[[0 2 0 0 1 1 0 0 0 0 0 0 0 0 1 2 0 1 1 2 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1\n",
      "  0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 2 1 0 1 1 1 0 1 0 0 0 0 0 0\n",
      "  0 0 1 1 2 4 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 3 2 1 0 0 1 0\n",
      "  1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 4\n",
      "  0 0 1 0 1 0 1 1 1 0 1 2 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 3 1 1 1]\n",
      " [3 0 1 2 1 3 1 1 1 4 1 1 1 1 0 2 1 1 0 1 0 3 0 2 1 0 2 1 1 0 1 4 1 0 0 0\n",
      "  2 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 2 0 0 2 0 0 0 1 1 1 2 1 1 1 1\n",
      "  2 1 0 0 3 2 1 1 1 0 3 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 3 2 1 5 2 1 1 1 0 1\n",
      "  0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 2 0 7\n",
      "  1 1 0 1 0 1 0 1 3 1 0 0 1 0 0 1 2 2 0 0 1 1 3 1 1 2 1 0 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bow = bow.toarray()\n",
    "print(bow.shape)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- We don't have control on the vocab size unlike keras\n",
    "\n",
    "- The matrix is (CSR) (Compressed Sparse Row), which is more compact representation for matrices with many zeros. The BoW is very sparse, it's a good way to represent it.\n",
    "You can easily recover the array when needed using `toarray()`\n",
    "\n",
    "- Vocab is all lower case, and punctuation is ignored. All those are configurable from sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
