{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\"\"\"Compute softmax values for each sets of scores in x.\"\"\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(value):\n",
    "    exponential_value = np.exp(value - np.max(value))\n",
    "    return exponential_value / exponential_value.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wordvector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_to_vector(object):\n",
    "    def __init__(self) :\n",
    "        self.values = 10\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 2\n",
    "        self.learning_rate = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "        #-----------------------------------------------\n",
    "    def initialize(self,v,data):\n",
    "        self.v = v\n",
    "        self.w = np.random.uniform(-0.8,0.8,(self.v,self.values))\n",
    "        self.w1= np.random.uniform(-0.8,0.8,(self.values , self.v))\n",
    "        self.words = data \n",
    "        for i in range(len(data)):self.word_index[data[i]] = i \n",
    "        #----------------------------------------------------------\n",
    "        #Create Data Forward Propagation Function \n",
    "    def feed_forward(self,x):\n",
    "        self.h = np.dot(self.w.T,x).reshape(self.values,1)\n",
    "        self.u = np.dot(self.w1.T,self.h)\n",
    "        print(self.u)\n",
    "        self.y = softmax(self.u)\n",
    "        return self.y \n",
    "        #----------------------------------------------------------------\n",
    "        #Create BackPropagatiion Function Because Make Update Weights\n",
    "    def backpropagate(self,x,t):\n",
    "        e = self.y - np.asarray(t).reshape(self.values,1)\n",
    "        dervative_w1 = np.dot(self.h,e.T)\n",
    "        X = np.array(x).reshape(self.values,1)\n",
    "        dervative_w = np.dot(X , np.dot(self.w1,e).T)\n",
    "        self.dervative_w1 = self.learning_rate * dervative_w1\n",
    "        self.dervative_w  = self.learning_rate * dervative_w\n",
    "        #----------------------------------------------------------------------\n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs+1):\t\t \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.x_train)): \n",
    "                self.feed_forward(self.x_train[j]) \n",
    "                self.backpropagate(self.x_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.values): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "            print('epoch' ,x , 'Loss = ',self.loss)\n",
    "            self.learning_rate *= 1/(( 1+ self.learning_rate * x))\n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            x= [0 for i in range(self.values)] \n",
    "            x[index] = 1\n",
    "            prediction = self.feed_forward(x) \n",
    "            output = {} \n",
    "            for i in range(self.values): \n",
    "                output[prediction[i][0]] = i \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True):\n",
    "                top_context_words.append(self.words[output[k]])   \n",
    "                if (len(top_context_words)>=number_of_predictions): break \n",
    "            return top_context_words \n",
    "        else : print (\"Word not found in dicitonary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class word2vec(object): \n",
    "\tdef __init__(self): \n",
    "\t\tself.N = 10\n",
    "\t\tself.x_train = [] \n",
    "\t\tself.y_train = [] \n",
    "\t\tself.window_size = 2\n",
    "\t\tself.alpha = 0.001\n",
    "\t\tself.words = [] \n",
    "\t\tself.word_index = {} \n",
    "\n",
    "\tdef initialize(self,V,data): \n",
    "\t\tself.V = V \n",
    "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "\t\t\n",
    "\t\tself.words = data \n",
    "\t\tfor i in range(len(data)): \n",
    "\t\t\tself.word_index[data[i]] = i \n",
    "\n",
    "\t\n",
    "\tdef feed_forward(self,X): \n",
    "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "\t\tself.u = np.dot(self.W1.T,self.h) \n",
    "\t\t#print(self.u) \n",
    "\t\tself.y = softmax(self.u) \n",
    "\t\treturn self.y \n",
    "\t\t\n",
    "\tdef backpropagate(self,x,t): \n",
    "\t\te = self.y - np.asarray(t).reshape(self.V,1) \n",
    "\t\t# e.shape is V x 1 \n",
    "\t\tdLdW1 = np.dot(self.h,e.T) \n",
    "\t\tX = np.array(x).reshape(self.V,1) \n",
    "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "\t\tself.W1 -=  self.alpha*dLdW1 \n",
    "\t\tself.W -= self.alpha*dLdW \n",
    "\t\t\n",
    "\tdef train(self,epochs): \n",
    "\t\tfor x in range(1,epochs+1):\t\t \n",
    "\t\t\tself.loss = 0\n",
    "\t\t\tfor j in range(len(self.X_train)): \n",
    "\t\t\t\tself.feed_forward(self.X_train[j]) \n",
    "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "\t\t\t\tC = 0\n",
    "\t\t\t\tfor m in range(self.V): \n",
    "\t\t\t\t\tif(self.y_train[j][m]): \n",
    "\t\t\t\t\t\tself.loss += -1*self.u[m][0] \n",
    "\t\t\t\t\t\tC += 1\n",
    "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss) \n",
    "\t\t\tself.alpha *= 1/( (1+self.alpha*x) ) \n",
    "\t\t\t\n",
    "\tdef predict(self,word,number_of_predictions): \n",
    "\t\tif word in self.words: \n",
    "\t\t\tindex = self.word_index[word] \n",
    "\t\t\tX = [0 for i in range(self.V)] \n",
    "\t\t\tX[index] = 1\n",
    "\t\t\tprediction = self.feed_forward(X) \n",
    "\t\t\toutput = {} \n",
    "\t\t\tfor i in range(self.V): \n",
    "\t\t\t\toutput[prediction[i][0]] = i \n",
    "\t\t\t\n",
    "\t\t\ttop_context_words = [] \n",
    "\t\t\tfor k in sorted(output,reverse=True): \n",
    "\t\t\t\ttop_context_words.append(self.words[output[k]]) \n",
    "\t\t\t\tif(len(top_context_words)>=number_of_predictions): \n",
    "\t\t\t\t\tbreak\n",
    "\t\n",
    "\t\t\treturn top_context_words \n",
    "\t\telse: \n",
    "\t\t\tprint(\"Word not found in dicitonary\") '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Make Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus_data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    training_data = []\n",
    "    sentences = corpus_data.split(\".\")\n",
    "    for i in range (len(sentences)):\n",
    "        sentences[i] = sentences[i].strip()\n",
    "        sentence = sentences[i].split()\n",
    "        x_value = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "        x_value = [word.lower() for word in x_value]\n",
    "        training_data.append(x_value)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Prepare Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences,w2v):\n",
    "    data ={}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in data: data[word] =1\n",
    "            else : data[word] +=1\n",
    "    value = len(data)\n",
    "    data = sorted(list(data.keys()))\n",
    "    vocab = {}\n",
    "    for i in range(len(data)) : \n",
    "        vocab[data[i]] = i \n",
    "    for sentence in sentences :\n",
    "        for i in range(len(sentences)):\n",
    "            center_word = [0 for x in range (value)]\n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [ 0 for x in range (value)]\n",
    "            for j in range(i-w2v.window_size , i + w2v.window_size):\n",
    "                if i != j and j >= 0 and j<len(sentence):context[vocab[sentence[j]]] +=1\n",
    "            w2v.x_train.append(center_word)\n",
    "            w2v.y_train.append(context)\n",
    "    w2v.initialize(value,data)\n",
    "    return w2v.x_train,w2v.y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Detect Training Data & Number of epochs\n",
    "6. Send Corpous Data --> because make preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'earth', 'revolves', 'around', 'sun'],\n",
       " ['the', 'moon', 'revolves', 'around', 'earth']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"The earth revolves around the sun. The moon revolves around the earth\"\n",
    "epochs = 1000 \n",
    "training_data = preprocessing(corpus)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Recall Word_to_vector class\n",
    "8. prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0, 0, 1],\n",
       "  [0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 1, 0, 0, 0]],\n",
       " [[0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 1],\n",
       "  [0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v =word_to_vector()\n",
    "prepare_data(training_data , w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22779061]\n",
      " [ 0.26483157]\n",
      " [ 0.28919225]\n",
      " [-0.64472373]\n",
      " [-0.99836706]\n",
      " [ 0.0757863 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 6 into shape (10,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w2v\u001b[39m.\u001b[39;49mtrain(epochs)\n",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m, in \u001b[0;36mword_to_vector.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_train)): \n\u001b[0;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_train[j]) \n\u001b[1;32m---> 40\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_train[j],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train[j]) \n\u001b[0;32m     41\u001b[0m     C \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues): \n",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mword_to_vector.backpropagate\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackpropagate\u001b[39m(\u001b[39mself\u001b[39m,x,t):\n\u001b[1;32m---> 28\u001b[0m     e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39;49masarray(t)\u001b[39m.\u001b[39;49mreshape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues,\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     29\u001b[0m     dervative_w1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh,e\u001b[39m.\u001b[39mT)\n\u001b[0;32m     30\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(x)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues,\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 6 into shape (10,1)"
     ]
    }
   ],
   "source": [
    "w2v.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,6) and (10,) not aligned: 6 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mw2v.predict(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maround\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,3) --> \u001b[39m\u001b[39m'\u001b[39m,w2v\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39maround\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mw2v.predict(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maround\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,5) --> \u001b[39m\u001b[39m'\u001b[39m,w2v\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39maround\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mw2v.predict(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msun\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,3) --> \u001b[39m\u001b[39m'\u001b[39m, w2v\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39msun\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m, in \u001b[0;36mword_to_vector.predict\u001b[1;34m(self, word, number_of_predictions)\u001b[0m\n\u001b[0;32m     52\u001b[0m x\u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues)] \n\u001b[0;32m     53\u001b[0m x[index] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 54\u001b[0m prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(x) \n\u001b[0;32m     55\u001b[0m output \u001b[39m=\u001b[39m {} \n\u001b[0;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues): \n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mword_to_vector.feed_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw\u001b[39m.\u001b[39;49mT,x)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues,\u001b[39m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw1\u001b[39m.\u001b[39mT,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh)\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,6) and (10,) not aligned: 6 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "print('w2v.predict(\"around\",3) --> ',w2v.predict(\"around\",3))\n",
    "print('w2v.predict(\"around\",5) --> ',w2v.predict(\"around\",5))\n",
    "print('w2v.predict(\"sun\",3) --> ', w2v.predict(\"sun\",3))\n",
    "print('w2v.predict(\"earth\",3) --> ', w2v.predict(\"earth\",3))\n",
    "print('w2v.predict(\"jupiter\",3)--> ', w2v.predict(\"jupiter\",3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
